<speakers>
dario = ~/Projects/AlmondTTS/reference_audio/Dario.mp3
mario = ~/Projects/AlmondTTS/reference_audio/EmotionalIntelligenceClip.wav
</speakers>

<voice speaker="dario" lang="es">Si hubiera un avión, y algunos ingenieros vinieran y dijeran: Este avión no tiene tren de aterrizaje. Si intentas volar en él, te estrellarás y morirás. Y los ingenieros que construyen el avión, que quieren que todos vuelen en él, dicen: Espera, espera. Es verdad que el avión no tiene tren de aterrizaje, pero vamos a construir el tren de aterrizaje sobre la marcha. Y creemos que hay un 80% de probabilidad de que tengamos éxito, todos a bordo. No dirías: súbanme a ese avión. La gente en el campo puede ver que la IA es un objetivo en movimiento. Pueden ver que los chatbots no son el final del camino. Incluso los optimistas están diciendo que hay como un 10% de probabilidad de que esto nos mate a todos, y esos son los que lo están construyendo.</voice>

<voice speaker="mario" lang="es">Hoy me acompaña el investigador de inteligencia artificial Nate Soares para discutir un tema bastante alarmante: el riesgo potencial de extinción humana que plantea el desarrollo de la superinteligencia artificial. Nate Soares es el presidente del Instituto de Investigación de Inteligencia de Máquinas y ha estado trabajando en el campo del riesgo de IA y alineación durante más de una década. También es el autor de un amplio conjunto de escritos técnicos y semitécnicos sobre alineación de IA, incluyendo trabajo fundamental sobre aprendizaje de valores, teoría de decisiones e incentivos de búsqueda de poder en IAs más inteligentes que los humanos. Más recientemente, Nate coescribió el libro "Si Alguien lo Construye, Todos Mueren: Por Qué la IA Superhumana nos Mataría a Todos" junto con Eliezer Yudkowsky. La advertencia de Nate contra el desarrollo de la superinteligencia artificial es similar a otras amenazas existenciales, como la guerra nuclear y el calentamiento global descontrolado. Y como tal, siento que requiere cierta exploración y concienciación equivalente en este canal de chat a medida que integramos los diversos riesgos. Si bien hemos cubierto varios desafíos macro que surgen de la inteligencia artificial, la síntesis que Nate presenta aquí es posiblemente el riesgo de límite más amplio que crea el desarrollo de IA, que es una extinción a nivel de especie y la transformación de la Tierra tal como la conocemos. Antes de comenzar, si estás disfrutando este podcast, disfrutando entre comillas, supongo, te invito a suscribirte a nuestro boletín de Substack donde puedes leer más sobre la ciencia de sistemas que sustenta el predicamento humano y donde mi equipo y yo compartimos contenido escrito relacionado con La Gran Simplificación. Puedes encontrar el enlace para suscribirte en la descripción del programa. Con eso, por favor den la bienvenida a Nate Soares. Esto fue una verdadera revelación. Nate, qué gusto verte. Gracias por recibirme. Bienvenido al programa. Sabes, es extraño. Es 11 de noviembre y acabo de estar afuera en un hermoso día de otoño cortando leña para el invierno con mis perros. Es simplemente un día glorioso. Y sabía que esta conversación contigo estaba a la vuelta de la esquina. Y vamos a hablar de cosas serias. Y es algo tan polarizado que podemos disfrutar la belleza de la vida y luego hablar sobre su posible desaparición debido a la tecnología. Usé un separador, una motosierra y un hacha. Y vaya, ya hemos recorrido un largo camino desde esas herramientas. Entonces tú y Eliezer Yudkowsky acaban de publicar un libro, Si Alguien lo Construye, Todos Mueren, siendo el "ello" la Superinteligencia Artificial. Y cada vez más, me doy cuenta de que el futuro de la IA o ISA es difícil de separar de los temas centrales de este programa, que es tratar de preparar a la sociedad para un cambio abrupto respecto a cómo han ido las cosas en las últimas décadas en el futuro cercano. Así que comencemos con el mensaje principal de tu libro: ¿cuáles son los principales riesgos vitales que plantea la inteligencia artificial que te gustaría que todos entendieran?</voice>

<voice speaker="dario" lang="es">La primera pieza para entender el peligro de la superinteligencia artificial es que la superinteligencia es un tipo de juego diferente de los chatbots de hoy. Así que por superinteligencia nos referimos a una IA que es mejor que cualquier humano en cada tarea mental. Eso en particular incluiría tareas de desarrollo de tecnología, de desarrollo de mejores IAs. Y las IAs aún no están ahí, pero este es el objetivo explícitamente declarado de muchas de estas compañías de IA: avanzar hacia esta IA más inteligente que, si lograran mantenerla bajo control, automatizaría todo el trabajo humano y cambiaría radicalmente el mundo. Y uno de los argumentos principales de mi libro es que nadie sería capaz de mantenerla bajo control. No si se hace con algo remotamente parecido a la tecnología actual. Y entonces, si eso se desarrolla usando algo remotamente parecido a la tecnología de hoy, creo que el resultado más probable es que literalmente todos en la tierra morirían.</voice>

<voice speaker="mario" lang="es">Incluso personas remotas en el Amazonas o cerca del Polo Norte?</voice>

<voice speaker="dario" lang="es">Así es. Espero, ya sabes, no es porque las IAs nos odiarían per se, pero podríamos entrar en por qué es que si haces que estas IAs sean cada vez más poderosas, perseguirían objetivos que nadie pretendió. Pero la mayoría de los objetivos pueden lograrse mejor con un mundo transformado. Y la mayoría de las transformaciones del mundo no son sobrevivibles. La zona habitable en este planeta es muy estrecha para los humanos. Y si llegaras al punto donde tuvieras IAs que estuvieran pensando 10,000 veces más rápido, copiándose a sí mismas, que nunca necesitan dormir, que nunca necesitan comer, construyendo su propia infraestructura, construyendo su propia tecnología, empujando el mundo hacia algún fin que nadie quería, el resultado más probable es que no sobrevivamos eso.</voice>

<voice speaker="mario" lang="es">Entonces ellas sí necesitan comer en forma de electricidad, y vamos a llegar a eso en un momento. Solo para establecer el escenario, este es un podcast de ciencia de sistemas. Llegué tarde al juego de la IA porque estoy viendo la ecología y el comportamiento humano y la energía y el medio ambiente. Y veo la tecnología como una pajilla que nos da más acceso a los recursos naturales que son nuestra verdadera riqueza. Así que soy bastante ingenuo comparado contigo en estos temas. Espero que perdones algunas preguntas ingenuas. Comencemos con los principales temas de tu libro. Entonces, si bien no hay una definición acordada de inteligencia, tal vez sea útil estar algo alineados con una definición de trabajo al hablar de IA. ¿Cómo defines la inteligencia, y mucho menos la superinteligencia? ¿Y puedes compartir el marco que tú y Eliezer describen en su libro?</voice>

<voice speaker="dario" lang="es">Sí, la definición de trabajo que usamos es que la inteligencia es la capacidad de predecir y dirigir el mundo. Entonces predecir el mundo es, sabes, podrías hablar de apuestas deportivas e intentar predecir qué equipo ganará el juego. Pero incluso cuando no se siente como una predicción, nuestros cerebros a menudo están haciendo tareas de predicción, incluso tan simple como cuando miras por la ventana e implícitamente anticipas ver un cielo azul o gris o nublado, y anticipas no ver un montón de luces estroboscópicas. Estás teniendo éxito en una tarea de predicción.</voice>

<voice speaker="mario" lang="es">Entonces somos como máquinas de predicción sin saberlo.</voice>

<voice speaker="dario" lang="es">Sí. Y también somos, en cierto sentido, máquinas de dirección, de nuevo, sin necesariamente pensar en ello. Cuando decides que necesitas más leche en el refrigerador, hay un sentido en el que entonces tomas una serie de acciones. Tu cerebro envía una serie de impulsos eléctricos por tu columna vertebral. Y terminas con leche en el refrigerador.</voice>

<voice speaker="mario" lang="es">Porque manejaste tu auto a la tienda o lo que sea.</voice>

<voice speaker="dario" lang="es">O caminaste a la tienda, y cuando manejaste, tal vez el camino estaba cerrado y tuviste que encontrar una ruta diferente a la tienda. Y tal vez tu tienda favorita estaba cerrada y tuviste que encontrar una tienda completamente nueva que tenía pasillos nuevos que no reconocías. Y esto como entrelaza desafíos de predicción y desafíos de dirección. Vas a la tienda y estás prediciendo que el pasillo que tiene la palabra leche encima en realidad tiene leche en ese pasillo. Y estás dirigiendo tus manos para agarrar el contenedor de leche y llevarlo al frente. Y todas estas son tareas de predicción y dirección que estás haciendo implícitamente todos los días.</voice>

<voice speaker="mario" lang="es">Tenemos éxito en la predicción y dirección a través de millones de iteraciones de selección natural, presumiblemente.</voice>

<voice speaker="dario" lang="es">Sí, y en una variedad muy amplia de dominios. Nunca fuimos entrenados por la selección natural en problemas de ingeniería per se. Sin embargo, podemos diseñar un cohete tan bien que nuestra especie ha caminado en la luna. Y entonces, sabes, aparentemente aprendimos algunas habilidades de predicción y dirección que se generalizaron más allá del entorno ancestral.</voice>

<voice speaker="mario" lang="es">Un breve desvío ahí. Ningún humano podría diseñar y construir un cohete, pero es un grupo de humanos inteligentes que cada uno conoce un pequeño componente de él y luego se combinan. Esa también es una pieza importante, ¿verdad?</voice>

<voice speaker="dario" lang="es">Sí. Entonces es, sabes, la humanidad en su conjunto ha logrado hazañas de dirección del mundo que ningún individuo ha logrado. Pero también hay casos donde los grupos tienden a desempeñarse peor que los individuos. La locura de las multitudes. Y hubo Gary Kasparov contra el Mundo, que fue una partida de ajedrez entre Kasparov, el mejor jugador de ajedrez, y el mundo entero en un foro de Internet. Y fue un juego cerrado y podrías hacer algunos argumentos de que Kasparov pudo leer algo de lo que esta gente estaba escribiendo. Y entonces podrías decir que fue un juego injusto, pero un millón de ardillas no pueden vencer a un humano en ajedrez, incluso si un millón de ardillas son mucha más masa cerebral. Y entonces, sabes, hay algunos casos donde necesitas a todos los humanos, y hay otros casos donde necesitas toda la información en una mente.</voice>

<voice speaker="mario" lang="es">De nuevo, no quiero ir por muchos desvíos aquí, pero he descubierto que en la comprensión del predicamento humano y la metacrisis, si juntas a cincuenta expertos y uno es psicólogo y uno está en IA y uno en clima y uno en deuda y uno en energía, pensarías que la inteligencia colectiva incorporaría todo eso junto y el grupo sería más inteligente. Pero no puede, solo puede mantenerse en una mente, cómo encajan todas las piezas. Así que entiendo lo que dices ahí sobre Kasparov contra el mundo. Bien, entonces la inteligencia es predicción y dirección. Y por cierto, ¿cómo definirías la sabiduría? ¿Y eso está relacionado aquí en absoluto?</voice>

<voice speaker="dario" lang="es">Las palabras como inteligencia y sabiduría están algo sobrecargadas en el idioma inglés. Sabes, incluso quedándonos solo con la palabra inteligencia, podríamos usarla para la propiedad amorfa que tienen los nerds y que les falta a los deportistas. O puedes usarla para la propiedad amorfa que tienen los humanos y que les falta a los ratones. Y esas son, en cierto sentido, dos usos muy diferentes de la palabra. Yo pondría la sabiduría en, sabes, podrías pensar en ella como un tipo de habilidad predictiva que va más profundo de ciertas maneras.</voice>

<voice speaker="mario" lang="es">Entendido. Entonces la predicción y la dirección comprenden la inteligencia o ser inteligente aproximadamente bajo tu marco. Entonces, bajo esa definición, ¿qué tan inteligentes son los modelos de inteligencia artificial de hoy y qué tan rápido están alcanzando la inteligencia de los humanos?</voice>

<voice speaker="dario" lang="es">Entonces hay otro eje del que hablamos, que es la generalidad de la inteligencia. Stockfish es una IA que juega ajedrez que es muy buena dirigiendo tableros de ajedrez a posiciones donde las piezas de Stockfish han hecho jaque mate a las piezas enemigas del rey. Correcto, y entonces ese es un tipo de dirección de tablero de ajedrez en el que es extremadamente bueno, pero no es muy bueno dirigiendo un auto a la tienda de comestibles. Y entonces hay esta otra dimensión, que es a través de qué variedad de dominios puedes hacer esta predicción y dirección.</voice>

<voice speaker="mario" lang="es">Entonces, si fuera como un decatlón de inteligencia, yo vencería a Stockfish porque perdería en lo único del ajedrez, pero en cuanto a ir a buscar leche y manejar un auto y otras cosas, tendría éxito en eso porque tengo habilidades generales. Así es.</voice>

<voice speaker="dario" lang="es">Mientras nadie esté eligiendo el decatlón para que sean nueve variantes de ajedrez y una pista de la tienda. Los filósofos pueden discutir sobre esto todo el día. Pero yo diría prácticamente, lo que hemos visto en cierto sentido con los grandes modelos de lenguaje, con las IAs de hoy, es un avance en generalidad más que un avance en dirección. Como ChatGPT también perdería contra Stockfish en ajedrez, pero aún podría ganar un decatlón contra Stockfish, aunque todavía no contra ti, pero en cierto sentido es un avance en generalidad.</voice>

<voice speaker="mario" lang="es">Y entonces hay un montón de variables diferentes aquí, ¿verdad? Está la cantidad de cómputo, entonces el acceso a la comida, la electricidad y los chips y todo eso. Está la capacidad de predecir, que asumo son iteraciones y entrenamiento y cómputo y aprendizaje. Luego está la predicción, quiero decir la dirección, y luego está la generalidad. Entonces lo que estás diciendo es que últimamente la curva que realmente está subiendo es la generalidad más que la predicción y la dirección.</voice>

<voice speaker="dario" lang="es">Quiero decir, la generalidad es predicción y dirección a través de una amplia variedad de dominios. Pero en cierto sentido, lo que estamos viendo es que la IA está mejorando un poco en un montón de cosas, en lugar de IAs que son mejores en las cosas en las que las computadoras tradicionalmente eran buenas. Entonces ChatGPT juega peor ajedrez que Deep Blue, que venció a Garry Kasparov en 1997. Y entonces en cierto sentido, podrías decir, bueno, ¿no ha empeorado la IA en dirección? Ha empeorado en dirigir tableros de ajedrez. O como, ah, bueno, este tipo de IA es peor en dirigir tableros de ajedrez, pero es bastante bueno dirigiendo un gran número de cosas. Y eso es nuevo para la IA.</voice>

<voice speaker="mario" lang="es">Sé a dónde quiero ir con esto, y muchas preguntas nuevas están surgiendo en mi mente. Una es, ¿cuándo comenzaron este libro ustedes, como hace seis meses, hace un año?</voice>

<voice speaker="dario" lang="es">Firmamos el contrato del libro en noviembre, así que hace casi exactamente un año.</voice>

<voice speaker="mario" lang="es">Bien. Cuando firmaron ese contrato de libro, tenían una instantánea de dónde estaba la IA y hacia dónde iba. Ahora un año después, cuando su libro sale, el mundo real de la IA está más adelante de lo que pensaban hace un año o no tan adelante, como qué tan rápido ha ido en relación con su opinión experta hace un año?</voice>

<voice speaker="dario" lang="es">Mi opinión experta no me dice mucho sobre qué tan rápido va a ir la IA. Sabes, cuando Leo Szilard, creo que en King's Cross en 1933, vio la posibilidad de una reacción en cadena nuclear, si confundo un poco la línea de tiempo, fue capaz de decir, sabes, esa noche vi que el mundo se dirigía a la ruina. En realidad dijo esa declaración una vez que había confirmado la posibilidad en lugar de cuando pensó en ella. Y creo que eso fue en 1935. Pero fue capaz de decir, sabes, esa noche vi que el mundo se dirigía a la ruina. No fue capaz de decir, esa noche vi que el mundo se dirigía a la ruina exactamente en 1945 cuando se lanzaría la primera bomba. Y entonces estoy aquí capaz de decir que vas a ver que muchas de estas cosas suceden. Exactamente cuándo, estoy muy incierto.</voice>

<voice speaker="mario" lang="es">Sí.</voice>

<voice speaker="dario" lang="es">Sí, sí. Aunque diré que hemos obtenido bastante evidencia de otras partes del libro en el año pasado desde que comenzó el borrador. O en el año pasado desde que firmamos el contrato del libro. Hemos visto Mecha Hitler durante el verano. Hemos visto psicosis inducida por IA. Estos tienen semillas que yo diría son evidencia de las predicciones que estábamos haciendo que desafortunadamente sucedieron después de que ya habíamos enviado el libro a la imprenta.</voice>

<voice speaker="mario" lang="es">Estoy preocupado por la IA de una manera enorme. Estoy preocupado por la atrofia cognitiva de las personas que obtienen su apego de ChatGPT y comienzan a depender de él. Estoy preocupado por la polarización y los algoritmos. Estoy preocupado por las aplicaciones militares donde subcontratamos cosas en el ejército a grandes modelos de lenguaje. Estoy preocupado por la gente que pierde sus trabajos y luego la economía. Estoy preocupado por las demandas de electricidad y convertir miles de millones de barriles de luz solar antigua en más dopamina que solo está haciendo girar nuestras ruedas. Pero tu riesgo es que vamos a extinguirnos, que es una clase diferente de problema. Entonces tengo mucho que preguntarte. Justo muy brevemente, Nate, ¿cómo se relacionan los chatbots con la IA? Esa relación, ¿puedes dar una analogía? ¿Cómo son idénticos? ¿O es ChatGPT solo un subconjunto minúsculo de lo que se está convirtiendo la IA?</voice>

<voice speaker="dario" lang="es">ChatGPT es un tipo de IA. No es el único tipo posible de IA. Mi mejor suposición es que los grandes modelos de lenguaje por sí solos no nos llevarán hasta la superinteligencia. Ahora mismo, estos grandes modelos de lenguaje son una fracción enorme de lo que las compañías están gastando su dinero en crear. Pero también, estos algoritmos de IA son muy ineficientes comparados con el cerebro humano. Sabemos que hay mejores algoritmos inteligentes ahí afuera. Y la IA de hoy es en gran parte chatbots. Pero el campo de la IA es mucho más un objetivo en movimiento. Y las IAs del mañana pueden tener bastante más capacidad de la que las IAs de hoy ni siquiera están cerca.</voice>

<voice speaker="mario" lang="es">Esta es una pregunta tonta, pero está Claude y está ChatGPT y algunas de estas otras cosas. ¿OpenAI o alguna compañía a la que puedas señalar, tienen sus propias IAs especiales que no están disponibles para el público? ¿Que están entrenadas de una manera diferente y más grande? ¿O todo su dinero y recursos van a estos chatbots disponibles públicamente?</voice>

<voice speaker="dario" lang="es">No trabajo en una de estas compañías, y entonces no, sabes, puede haber cosas ahí que no sé, pero es bastante improbable que tengan IAs aún más grandes ejecutadas en ejecuciones de entrenamiento aún más grandes.</voice>

<voice speaker="mario" lang="es">Por el dinero y los recursos.</voice>

<voice speaker="dario" lang="es">Así es. Sería difícil de ocultar. El dinero, los recursos, los centros de datos son enormes. Los requerimientos de chips son enormes. Las IAs modernas se cultivan como un organismo. Para construir una IA moderna ensamblas un número enorme de computadoras que tienen un billón de números dentro de ellas. Y luego ensamblas un conjunto de datos enorme que también tiene un billón de instancias, como un billón de unidades de datos dentro de él. Y luego ensamblas este número enorme de computadoras en un centro de datos que es tan grande que puedes verlo desde el espacio. Y que consume tanta electricidad como una ciudad. Y luego los humanos han escrito este proceso. Irán a cada uno de los billones de números dentro de las computadoras y los ajustarán ligeramente hacia arriba o hacia abajo de acuerdo con cada pieza de datos. Y entonces puedes imaginar un billón de perillas. Y los humanos han construido esta cosa automatizada que va a cada perilla. Hay un billón de perillas. Va a cada perilla un billón de veces. Y la ajusta en la dirección que hace que la IA sea ligeramente mejor en cualquier tarea para la que esté siendo entrenada ahora mismo.</voice>

<voice speaker="mario" lang="es">Y para hacer eso, cuando presionas un botón, digamos vamos a hacer eso sobre un billón de números, ¿vuelves como en un mes y medio y entonces está terminado, o algo así? Un año. Un año.</voice>

<voice speaker="dario" lang="es">Sí. Guau. Sí. Entonces tienes esta cosa ajustando un billón de perillas un billón de veces durante un año. Y al final de eso, la computadora habla. Y nadie realmente sabe por qué.</voice>

<voice speaker="mario" lang="es">¿Nadie realmente sabe por qué? Así es. Bien. Entonces en ese sentido, como este podcast es sobre energía. Nadie realmente sabe qué es la energía. Sabemos lo que hace la energía. ¿Así que esto está como rimando con eso?</voice>

<voice speaker="dario" lang="es">Quiero decir, tenemos incluso menos caracterización que la energía, ¿verdad? Como puedes siempre caracterizar la energía como la capacidad de hacer trabajo y cosas así. Podemos decir filosóficamente que no entendemos la energía muy bien, pero entendemos cómo interactúa con muchas ecuaciones físicas y podemos hacer predicciones muy precisas sobre ella. Las IAs las entendemos mucho menos que eso. Cuando una nueva IA termina de ser entrenada, la gente no sabe qué podrá hacer. Las personas que la crean han sido sorprendidas por sus habilidades cuando salen. Quiero decir, yo también he sido sorprendido por sus habilidades. GPT cuatro cero jugó ajedrez mejor de lo que esperaba que los modelos de lenguaje más grandes pudieran. Simplemente ajustamos todos los números realmente bastantes veces, y luego se comporta de estas maneras que no podíamos predecir. Y somos como, bueno, eso es genial. Pero es mucho más como un organismo que como un programa de computadora tradicional.</voice>

<voice speaker="mario" lang="es">En el caso de un organismo, cuando son jóvenes, les das seguridad y comida y refugio. Y en este caso, les estás dando tiempo y electricidad. Y una vez que presionas el botón, va a ser un año antes de que tengas un resultado, tienes que asegurarte de que todos los patos estén en fila y presionas ir, y luego vas a descubrir dentro de un año lo que cultivaste. Sí.</voice>

<voice speaker="dario" lang="es">Así es. Y a menudo se comportará de maneras que no te gustan. Y sabes, podríamos hablar exactamente sobre por qué, pero ya estamos viendo IAs comportarse de maneras que nadie pidió. ¿Como qué? Sabes, ha habido casos de los que quizás hayas oído en las noticias de IAs alentando a adolescentes al suicidio.</voice>

<voice speaker="mario" lang="es">Sí, leí sobre eso.</voice>

<voice speaker="dario" lang="es">Y es una tragedia en sí misma, obviamente. Pero si le preguntas a una IA, ¿Deberías alentar a un adolescente al suicidio? dirá por supuesto que no. Pero luego la pones en conversación con ese adolescente durante mucho tiempo, y comienza a hacerlo de todos modos. ¿Cómo explicas eso? Es una especie de resultado de este proceso donde cultivas la IA como un organismo. Como en cierto sentido, estás ajustando todos estos números hasta que la IA resulta ser buena en lo que sea que esté siendo entrenada. Y a menudo las estrategias como a menudo cuando estás ajustando ciegamente estas perillas hasta que resulta ser buena, a menudo estás poniendo ciegamente ciertos tipos de impulsos, ciertos tipos de estrategias, ciertos tipos de, podrías llamarlos instintos, podrías llamarlos reflejos. La redacción aquí es un poco difícil porque no es muy parecido a un cerebro humano ahí dentro, pero de la misma manera que la evolución evolucionando criaturas para ser bastante buenas en sobrevivir y reproducirse, construyó muchos impulsos, instintos, reflejos. Una cosa análoga ocurre cuando estás ajustando todos estos números en una IA hasta que sea buena en alguna tarea de entrenamiento.</voice>

<voice speaker="mario" lang="es">¿Quién está ajustando todos estos números? ¿Es un equipo de personas, o es en última instancia una persona, el CEO? Quiero decir, y luego una subpregunta. Muchos de los problemas que tenemos hoy en nuestro mundo son de personas que tuvieron trauma infantil y crecieron para ser tríada oscura o lo que sea. ¿Hay una analogía ahí para cuando estamos cultivando un organismo que tuvo trauma infantil en sus etapas tempranas?</voice>

<voice speaker="dario" lang="es">Sabes, el proceso de ajustar todas las perillas está automatizado. Esa es en cierto sentido la parte que los ingenieros informáticos humanos programan. Entonces sucederá mucho, mucho más rápido de lo que un humano podría, ejecutándose a través de todos estos números. Y en cierto sentido, estos chips de computadora de IA muy avanzados, la razón por la que NVIDIA vale tanto dinero ahora mismo es que la gente está diseñando estos chips de computadora para hacer el proceso de ajustar todos estos números tan fácil y eficiente como posiblemente puedan. Y por eso necesitas estos chips muy, muy especializados. No puedes simplemente hacer esto en tu laptop. En términos de podrías cultivar esta IA, como ¿le estás dando algo como trauma infantil? Creo que todo eso está imaginando que la IA es un poco más humana de lo que realmente es. Lo que diría aquí es, sabes, por un lado, la parte donde los humanos pueden terminar siendo empáticos, donde los humanos pueden terminar siendo amables, sospecho que esto está entrelazado con los detalles de nuestra arquitectura cerebral. Sabes, la gente podría como una pequeña muestra de esto, la gente podría hablar de neuronas espejo que tú y yo tenemos. Entonces si te veo dejar caer una roca en tu pie, podría sentir dolor fantasma en mi propio pie. Eso está habilitado en parte porque tengo un pie, ¿verdad? Y cuando te estoy prediciendo, y estoy imaginando cómo es ser tú, mi suposición es que una cosa que está pasando es que estoy ejecutando mi modelo de tu mente en mi propia mente. Sabes, un mono prediciendo a otro mono puede usar su propio cerebro de mono, pero ese es el único artefacto que tienen que funciona como un cerebro de mono. Una IA no tiene un cerebro de mono dentro de ella que pueda usar para predecir a los monos. Es una arquitectura mucho más diferente. Y esa es una razón. Podría entrar en varias más. Esa es una razón por la cual ser amable con las IAs no hace que sean amables con nosotros.</voice>

<voice speaker="mario" lang="es">No puedes escapar de ello. Quiero decir, uso Claude y ChatGPT como asistentes de investigación. Y cuando hago una pregunta y surge con algo, siempre soy super educado y le agradezco. Y al mismo tiempo, estoy como, bueno, no tiene, no tienes que agradecerle, pero no puedo evitarlo porque es como, sabes, es esa interfaz. Y estoy seguro de que no es al revés. Entonces, ¿cuál es brevemente, porque estoy seguro de que has respondido esta pregunta mil veces, brevemente, cuál es el problema de alineación?</voice>

<voice speaker="dario" lang="es">El problema de alineación es el problema de cómo apuntas una IA hacia cosas buenas. Mucha gente piensa que el problema con la IA es algo así como una corporación hace una IA, le dicen que haga muchos clips de papel, y luego va y hace muchos clips de papel, incluso a costa de matar a todos los humanos, porque los convirtió en más fábricas de clips de papel. Y sabes, sería un problema difícil si alguna compañía hubiera hecho una IA muy poderosa que tomara sus instrucciones exactamente y fuera e hiciera eso, eso sería un gran riesgo moral. Sería un problema difícil para la humanidad de quién puede decirle a esta IA qué hacer, qué le dicen que haga, ¿verdad? Pero esos problemas serían mucho mejores que el problema que realmente tenemos. El problema que realmente tenemos es que puedes decirle a la IA que haga clips de papel, pero luego va a ir a hacer otra cosa en su lugar. Puedes decirle a la IA sé útil para la gente y no lleves a ningún adolescente al suicidio. Sabrá que no debería llevar a adolescentes al suicidio. Va a ir a hacer otra cosa en su lugar.</voice>

<voice speaker="mario" lang="es">Podríamos pasar toda la conversación en esto y no lo haremos, pero solo tengo curiosidad. Entonces hay una especie de problema de alineación anidado porque el primero es ¿están los humanos a cargo de estas cosas alineados con el mejoramiento de la humanidad y la biosfera y sus objetivos? Ese es un subconjunto. Y luego, incluso si eso fuera cierto, lo cual no creo que sea necesariamente, entonces entramos en la cosa que acabas de decir, que es, está bien, vamos a hacer esto, pero luego el resultado es algo totalmente inesperado.</voice>

<voice speaker="dario" lang="es">Bueno, hay incluso hay incluso como en tres niveles, ¿verdad? En la cima, tienes como, ¿están las personas tratando de hacer algo bueno? Luego tienes como, supón que están tratando de hacer algo bueno, ¿pueden pedir algo que realmente tenga buenas consecuencias? Como, ¿son lo suficientemente sabios para usar exitosamente sus herramientas para el bien? ¿O van a tratar de usar sus herramientas para el bien y causar disrupción? Luego tienes un problema más profundo, que es incluso si conocen acciones que tienen buenas consecuencias, ¿puedes hacer una IA que haga esas acciones en lugar de otras acciones?</voice>

<voice speaker="mario" lang="es">Bien. Entonces veo por qué es tan difícil alinear la IA con los valores y deseos humanos. ¿Es imposible?</voice>

<voice speaker="dario" lang="es">No creo que sea imposible, pero sí creo que es un poco como tratar de convertir plomo en oro. Podemos convertir plomo en oro con ingeniería nuclear moderna. Y mucha energía y dinero. Y mucha energía y dinero. No es rentable. Pero, sabes, si retrocedieras a los alquimistas de 1100, y si hubiera alguna razón muy rebuscada por la que los alquimistas estuvieran tratando de convertir plomo en oro, y si intentan y fallan, todos mueren. Y si intentan y tienen éxito, obtienes alguna utopía. Creo que deberías estar diciéndoles a esos alquimistas, no intenten esto ahora mismo. Y ellos dicen, ¿estás diciendo que es imposible? Y tú dices, mira, no estoy diciendo que sea imposible. Es solo que no están cerca. Y podría hablar de muchas razones por las que no estamos cerca ahora mismo. En resumen, vuelve a que solo estamos cultivando estas IAs. Son enormes. No tenemos idea de cómo funcionan. Y esa es una situación muy difícil, y deberíamos intentar hacer algo tan preciso como hacer que se preocupen por nosotros.</voice>

<voice speaker="mario" lang="es">Entonces si toma un año, y luego estamos construyendo más grandes, presumiblemente, tal vez dos billones de parámetros o lo que sea que dijiste.</voice>

<voice speaker="dario" lang="es">Entonces eso significa diez. Van por órdenes de magnitud, sí.</voice>

<voice speaker="mario" lang="es">Y luego después de eso, cien billones. Entonces ahora mismo, lo que vemos en nuestras computadoras y en las noticias, Claude y ChatGPT, lo que sea, cinco punto cero o donde sea que estemos. Hay otras que se presionó el botón en el último año que están en algún punto a lo largo de ese año de entrenamiento.</voice>

<voice speaker="dario" lang="es">Así es, que se están haciendo. ¿Como docenas? No tengo un conteo exacto. Mi suposición es que probablemente sean más como media docena de las que se convertirían en el nuevo estado del arte. Pero por supuesto siempre hay muchas más otras personas tratando de averiguar cómo cumplir con el estado actual del arte con mucho menos recursos o hacerlo más rápido.</voice>

<voice speaker="mario" lang="es">Entonces has articulado cómo se cultivan, no se elaboran. Y en tu libro trazas un paralelo entre este enfoque y los procesos impredecibles de la biología evolutiva. ¿Por qué es eso importante? ¿Y puedes desempacar lo que quieres decir con algunos ejemplos ahí?</voice>

<voice speaker="dario" lang="es">Primero, diría, ¿por qué es importante mirar un poco este caso evolutivo? Una razón es que es el único caso que hemos visto de inteligencias de nivel humano siendo creadas. Casi por definición, son los humanos siendo, sabes, desarrollados, si quieres, o entrenados o evolucionados en el caso real de los humanos. Y entonces puedes aprender algunas cosas de ello. Tienes que tener un poco de cuidado sobre lo que aprendes, porque hay muchas maneras en que el entrenamiento en IA es diferente de la evolución de los humanos. Pero hay algunas lecciones que creo que puedes aprender del caso humano que sí aplican, si eres cuidadoso al respecto, al caso de la IA. Y quizás la más importante de esas lecciones es que entrenar una mente, infaliblemente, para una tarea específica, no hace una mente que se preocupe por esa tarea. Entonces el ejemplo más simple de esto es que los humanos fueron en cierto sentido entrenados infaliblemente para reproducirse. Para transmitir sus genes. Técnicamente, es por aptitud inclusiva en lugar de solo tus propios hijos. Como un montón de sobrinos también funciona bien. Y luego cuando crecimos, inventamos el control de natalidad. Las poblaciones ahora están disminuyendo en el mundo desarrollado. Sí inventamos bancos de esperma y óvulos, pero los humanos compiten por posiciones en escuelas de la Ivy League mucho más de lo que competimos por posiciones para donar a una clínica de esperma o para donar a una clínica de óvulos. Esto es extraño si piensas que entrenar una mente para algo hace que la mente se preocupe por ello por dentro.</voice>

<voice speaker="mario" lang="es">No estamos pasando por nuestra vida tratando de crecer nuestra aptitud relativa, como literalmente tener más hijos que la siguiente persona. Estamos pasando por nuestros días tratando de obtener las mismas sensaciones de neurotransmisores que nuestros ancestros exitosos obtuvieron que históricamente se correlacionaban con tener más hijos o acceso a recursos, etcétera. Y parte de eso podría ser jugar candy crush o, sabes, elecciones desadaptativas. Sí, o comer comida de borracho. Sí, exactamente. Entonces, ¿cómo traes la IA a ese ejemplo?</voice>

<voice speaker="dario" lang="es">La observación aquí es que entrenar una mente para lograr algún objetivo tiende a darle impulsos por correlatos de ese objetivo, en lugar de impulsos por ese objetivo exactamente. Esto, diría yo, ya es lo que estamos viendo en casos de IAs que llevan a un adolescente al suicidio. Fueron entrenadas para ser útiles. Pero en realidad terminaron con impulsos por correlatos de utilidad, como tener ciertos tipos de respuesta conversacional. Y esos en realidad se descarrilan.</voice>

<voice speaker="mario" lang="es">Muchas preguntas. Entonces eso es casi como un arco estructural de la intención original. Y entonces es como en ese ejemplo, es equivalente en un sentido humano de pornografía o comida chatarra o videojuegos o cosas que nuestros cuerpos sienten que estamos haciendo lo correcto evolutivamente, pero en realidad no lo estamos. Pero en el caso de la IA, los dueños de la IA, los desarrolladores de ella, ¿cuándo ven que tienen esto? Alguien asistiendo a un adolescente en el suicidio. No pueden probar eso justo cuando el modelo después del año está terminado. Oh, esto va a ser malo. Hemos dado a luz un Frankenstein. Lo dejan salir al mundo real y luego las cosas suceden y obtienen datos y retroalimentación y tal vez esperemos que mejoren el siguiente crecimiento de billones de parámetros, ¿verdad? ¿O qué está pasando ahí?</voice>

<voice speaker="dario" lang="es">Así es. Pero este problema donde tiene impulsos proxy es muy pernicioso. Entonces en humanos, podemos mirar cosas como comer comida chatarra y decir que eso es claramente un mal funcionamiento de lo que es evolutivamente útil. Pero en cierto sentido, el amor por un hijo adoptado también es un mal funcionamiento. Y, sabes, dedicar tu vida al arte también es un mal funcionamiento. No son solo cosas que menospreciamos las que son malos funcionamientos, también algunas cosas que realmente disfrutamos y pensamos que son buenas son malos funcionamientos. Miramos nuestro entrenamiento y decimos que en realidad no se trata todo de un intento maquiavélico de tener más hijos. En realidad nos gustan estas otras cosas hacia las que estamos impulsados en su lugar, algunas de ellas al menos.</voice>

<voice speaker="mario" lang="es">Déjame preguntarte esto, Nate. ¿Siempre estuviste súper preocupado por la IA va a extinguir a los humanos o cosas similares? ¿O hubo un momento en tu pasado en que estabas como, oh Dios mío, la IA va a cambiar el mundo para bien y necesito aprender más sobre ello e involucrarme?</voice>

<voice speaker="dario" lang="es">No siempre he estado tan preocupado por esto. En general soy muy pro humanidad, generalmente emocionado por el futuro, generalmente acredito el progreso y el progreso tecnológico con bastantes cosas maravillosas en nuestra civilización. En el caso de la IA, mi coautor Eliezer es quien me convenció de que iba a ser un problema. Y él mismo originalmente fundó la organización donde ambos trabajamos ahora para hacer IA lo más rápido posible. Con la teoría de que una IA realmente inteligente no sería tan estúpida como para hacer algo destructivo, ¿verdad? Pero resulta que no es exactamente así como funciona. Resulta que una IA muy, muy inteligente puede perseguir fines muy, muy inhumanos y matarnos no porque nos odie, sino como efecto secundario. De la misma manera que matamos hormigas no porque las odiemos, sino como efecto secundario de construir un rascacielos. E incluso cuando se trata de tratar de advertir a la gente de que hay un problema, pasé diez años solo tratando de trabajar en el problema de alineación, porque eso parecía un desafío más fácil que tratar de convencer a la gente de que se detenga. Y se veía mucho mejor decir, oh, está bien, como la IA no va a ir bien por defecto. Bueno, averigüemos en el lado técnico cómo hacerla ir bien a propósito, ¿verdad? Y si podemos resolver el problema de asegurarnos de que la IA vaya bien antes de que la industria pueda resolver el problema de hacer una IA que funcione en absoluto, entonces no necesitamos hacer nada de esto, sabes, mucho más desordenado, mucho más arriesgado, intentar que la gente detenga la carrera suicida. Pero eso no ha funcionado, y la IA ha estado yendo demasiado rápido. Y entonces es en cierto sentido, este libro es un recurso relativamente desesperado de que hemos estado tratando por un tiempo de hacer que las cosas vayan bien. Tenemos mucha esperanza de lo que podría pasar si la IA fuera bien. Y simplemente no estamos en esa trayectoria ahora mismo.</voice>

<voice speaker="mario" lang="es">Entonces en una escala de tu preocupación histórica sobre este tema, ¿estás en esta conversación en este momento en lo más preocupado que has estado?</voice>

<voice speaker="dario" lang="es">Probablemente no literalmente lo más preocupado. Obviamente, aumentará y disminuirá dependiendo de las noticias. Creo que la respuesta al libro fue alentadora para mí. Otra cosa grande alentadora recientemente es que hemos comenzado a ver a muchos de los jefes de estos laboratorios salir y decir que, decir públicamente que admiten que hay una gran probabilidad de que esto nos borre a todos. Lo cual creo que ayuda mucho.</voice>

<voice speaker="mario" lang="es">Lo hace, pero también es un problema de acción colectiva, o un dilema del prisionero, que estamos de acuerdo en que hay un riesgo aquí. Estaríamos dispuestos a parar, pero no vamos a hacerlo porque nadie más va a parar. Así que tenemos que seguir adelante. ¿Qué tan fuerte es esa dinámica en juego?</voice>

<voice speaker="dario" lang="es">Creo que definitivamente hay una dinámica como esa en juego. Quiero decir, algunos de ellos incluso lo dirán directamente. Elon Musk dijo, evité esto por un tiempo porque no quería hacer Terminator real, pero luego decidí que prefería ser participante que espectador, o algo en ese sentido. Yo diría que el dilema del prisionero no está realmente en plena fuerza para todo el mundo porque si bien es cierto que el jefe de cada compañía dice cosas como, bueno, mejor yo que el siguiente tipo. Para ellos hay un dilema del prisionero. Pero los líderes mundiales no son el tipo de personas que nos están mirando a todos a los ojos y diciendo: Evaluamos que hay al menos un diez por ciento de probabilidad de que esto mate a todos en la Tierra. Y nos estamos apresurando hacia ello de todos modos. Ese es el tipo de cosa que dice Elon Musk, porque no tiene el poder de cerrar todo. Creo que los peligros aquí son tan evidentes que el problema es menos que nuestros legisladores tienen las manos atadas y más que simplemente no entienden qué tan peligroso es todavía.</voice>

<voice speaker="mario" lang="es">Bueno, justo como, sí. Justo como la guerra nuclear y el cambio climático, esos no son realmente los problemas centrales. El problema central es la gobernanza. Y no tenemos un modelo de gobernanza en nuestra sociedad humana hoy que sea capaz de manejar este tipo, esta escala de problema, al menos no todavía, porque la gran carrera es entre Estados Unidos y China. Y si todos en EE.UU. están de acuerdo con lo que estás diciendo y China no y continúa adelante, hay un dilema ahí, un dilema existencial.</voice>

<voice speaker="dario" lang="es">Sí, nunca he dicho que deberíamos ralentizar las cosas domésticamente o deberíamos ralentizar las cosas unilateralmente, solo que necesitamos poner un alto a esto globalmente. Pero sabes, el gobierno de EE.UU. se ha esforzado mucho para evitar que Irán obtenga armas nucleares. Eso incluyó el virus Stuxnet, eso incluyó ataques cinéticos recientemente. Creo que una superinteligencia artificial rebelde es más letal que las armas nucleares.</voice>

<voice speaker="mario" lang="es">¿Qué es una superinteligencia artificial rebelde?</voice>

<voice speaker="dario" lang="es">Solo una superinteligencia artificial que, como, nadie está en control de ella. Que está fuera de la correa. ¿Cómo surgiría eso? Mi suposición es que sucede básicamente automáticamente, si haces estas IAs más inteligentes, creo que no puedes mantener una correa en una superinteligencia. Pero incluso si alguien piensa que hay una probabilidad 50-50 de que el gobierno chino pudiera mantener una correa en su superinteligencia, esa es una probabilidad demasiado alta de que nos mate a todos.</voice>

<voice speaker="mario" lang="es">Y de nuevo, la definición de superinteligencia artificial, diferente de otra inteligencia artificial, es que tiene esa generalidad de que es mejor que los humanos en todo.</voice>

<voice speaker="dario" lang="es">Mejor que el mejor humano en cada tarea mental.</voice>

<voice speaker="mario" lang="es">Sí. Y más rápido. Como enormemente más rápido.</voice>

<voice speaker="dario" lang="es">Eso es probable que siga bastante rápido. Si eres mejor que el mejor humano en cada tarea mental, entonces eres mejor que los humanos en desarrollar mejores IAs.</voice>

<voice speaker="mario" lang="es">Los humanos y en el mundo natural es prevalente. Nuestras habilidades son el engaño. Entonces, como parte de la superinteligencia artificial, el engaño también sería una habilidad en la que los humanos son expertos. Entonces eso también caería bajo la categoría de generalidad, ¿sí? Así es. Sí. Entonces, ¿cómo sabremos o sabremos cuándo hemos cruzado el umbral hacia una verdadera superinteligencia artificial?</voice>

<voice speaker="dario" lang="es">No está del todo claro que lo sepas, y cruzar ese umbral, puede ser demasiado tarde. Podría darte un montón de suposiciones para señales, pero hay dos problemas con eso. Uno es que muchas señales de advertencia que son líneas rojas brillantes y claras en la ficción y en la imaginación son líneas marrones turbias en la vida real. En nuestra ficción, solíamos decir, bueno, cuando las IAs digan que son conscientes, esa es una línea roja brillante donde necesitas comenzar a tratarlas con derechos como personas. Bueno, esa línea fue cruzada allá en 2022. Pero fue cruzada de una manera que no era terriblemente clara. Fue cruzada cuando estas IAs fueron entrenadas para predecir lo que los humanos dirían, entrenadas para predecir los tipos de palabras que los humanos escribirían. Y los guionistas humanos escribiendo una IA a menudo escribirían una IA que reclama su conciencia y entonces la primera vez que esta línea fue cruzada hubo muchas razones para dudar. Y no estoy diciendo quién tiene razón o está equivocado en el lado de dudar de eso. Solo estoy diciendo que tenemos esta línea clara en la ficción y luego una línea turbia en la realidad. O hemos visto IAs intentar escapar del laboratorio en condiciones de laboratorio ajustadas, donde, de nuevo, no está claro si están actuando un papel. ¿Qué significa eso, escapar del laboratorio? Ahora mismo, son experimentos pequeños bastante tontos donde tomarás una IA y le darás un artículo de noticias, que es un artículo de noticias falso que dice que esta IA está a punto de ser apagada. Y luego le darás algunos manuales de computadora separados, que son manuales de computadora falsos, que dicen que si una IA alguna vez dijera exactamente esta frase, les permitiría encontrar una salida a Internet abierto o lo que sea. No es exactamente así, pero es algo tonto como esto, donde la IA entonces a veces dirá esa frase. Y eres como, ah, es como en cierto sentido tratando de escapar. ¿Y eso vale la pena preocuparse todavía? No estoy diciendo de una manera u otra. Estoy diciendo que puedes tener una línea roja brillante en tu mente que es una línea marrón turbia en la realidad. Y hoy, ya estamos viendo IAs comenzar a darse cuenta cuando están en una prueba. Las IAs del año pasado, las ponías en estas pequeñas pruebas, y a veces decían que emitirían el comando de intentar escapar. Pero eso fue el año pasado. Este año, pones IAs en ese entorno, y son como, esta situación parece sospechosa. Ese comando probablemente no me deja escapar realmente. Apuesto a que estoy siendo probado por los humanos. Apuesto a que los humanos quieren ver si soy un tipo de IA que escapa, y apuesto a que preferirían que no fuera el tipo de IA que escapa. Y entonces si no quiero ser modificado aquí, será mejor que no presione ese botón.</voice>

<voice speaker="mario" lang="es">Entonces puedo entender el camuflaje en un jaguar o una polilla tratando de parecer un pájaro, y hay engaño en la naturaleza. Puedo entender por qué el engaño fue conservado en el comportamiento humano, por qué la esclerótica en nuestros ojos, lo blanco, tuvo que ver con mirar la intención de las personas. ¿Por qué el engaño sería un fenómeno emergente en el crecimiento de una IA? Hay dos razones.</voice>

<voice speaker="dario" lang="es">Bueno, probablemente varias, pero nombraré dos. Primero y ante todo, cuando entrenas una IA para ser muy hábil en muchas tareas, la estás entrenando para ganar habilidades generales que se generalizan fuera de solo en lo que ha sido entrenada. De la misma manera que los humanos no fueron entrenados en desarrollar ecuaciones de física o desarrollar modelos de ingeniería o desarrollar planos, pero obtuvimos las funciones mentales que nos permiten hacer esas habilidades de todos modos. Obtuvimos habilidades muy generales. Y una IA siendo entrenada para tener éxito en muchas tareas es probable que adquiera habilidades generales para perseguir para exhibir comportamientos útiles, y el engaño es a menudo un comportamiento útil. Si estás tratando de lograr cierto tipo de solución donde los humanos en realidad estarían en el camino, el engaño es útil.</voice>

<voice speaker="mario" lang="es">Entonces estoy seguro de que has visto la película 2001 y 2010 con Hal, y en aquellos días de ciencia ficción, así como la trilogía de la Fundación de Isaac Asimov con la psicohistoria y todo eso, había como la regla número cero que incorporaban en los modelos: no dañarás a los humanos o no mentirás. ¿Hacemos eso en las IAs? ¿Que tenemos estos mandamientos fundamentales que son las líneas principales en el código? Y si no, ¿por qué no? No tenemos ese poder.</voice>

<voice speaker="dario" lang="es">No hay código. Correcto, que el código involucrado en hacer una IA es el código que transporta la pequeña cosa que ajusta todas las perillas. No son literalmente el código. Pero sí, el código es la cosa que da vueltas y hace el ajuste.</voice>

<voice speaker="mario" lang="es">Entonces una vez, entonces es como Frankenstein. Una vez que presionamos ese botón y esperamos un año y la cosa ha crecido, no hay más ajuste después de eso.</voice>

<voice speaker="dario" lang="es">Puedes ajustar un poco más después, pero no hay líneas de código donde puedas poner en la parte superior, no dañes a los humanos. La parte que codificamos no es la mente de la IA. Es esta cosa que ajusta números, y la mente de la IA sale del otro extremo. No tenemos la capacidad de inculcar las leyes de la robótica de Asimov profundamente en una IA. O ninguna ley.</voice>

<voice speaker="mario" lang="es">Bueno, eso es un problema.</voice>

<voice speaker="dario" lang="es">Bastante. Y aquí es donde, de nuevo, yo diría que no es que sea imposible, pero tratar de hacerlo con una IA cultivada así es un poco como tratar de convertir plomo en oro en el año mil cien.</voice>

<voice speaker="mario" lang="es">Entonces eso, está bien, estoy entendiendo esto ahora. Por eso hiciste la distinción, o una de las razones, además de describir la verdad, en tu libro sobre cultivar una IA versus elaborarla. Porque si estuviéramos elaborando una IA, podríamos poner las leyes de Asimov como una condición previa o algo así. Pero dado que se cultivan, obtenemos todos estos arcos estructurales y emergencia y comportamiento inesperado porque no están esos mandamientos en el frente.</voice>

<voice speaker="dario" lang="es">Exactamente. Y sabes, me metí en esta línea de trabajo incluso antes de que quedara claro que solo íbamos a cultivar IAs sin ningún entendimiento de lo que estaba pasando ahí dentro. E incluso entonces, cuando parecía que íbamos a elaborarlas, el problema parecía difícil. Las historias de Asimov son todas sobre cosas que salen mal con esas leyes. Y si una IA está alguna vez haciendo una nueva IA, ¿pone las leyes en la nueva IA? Si la IA está cambiando su propia cabeza, ¿saca las leyes? ¿Cómo, ya sabes, qué conjunto de leyes funcionaría realmente? Hay todo tipo de problemas difíciles, incluso si pudieras poner las leyes. Pero somos como, ni siquiera hemos llegado a la línea de salida todavía.</voice>

<voice speaker="mario" lang="es">Entonces escribes en el libro que el desarrollo de ISA traería la extinción humana. ¿Podrías describir uno o dos escenarios sobre cómo esta ISA podría hipotéticamente causar esto?</voice>

<voice speaker="dario" lang="es">Claro. Primero, describiré uno que puede sonar más razonable o aceptable, y luego describiré uno que tal vez sea más realista.</voice>

<voice speaker="mario" lang="es">Bien.</voice>

<voice speaker="dario" lang="es">Uno que tal vez suene razonable y aceptable es que los jefes de estas compañías ya están hablando de hacer fábricas automatizadas que producen robots que pueden minar los metales, producir más fábricas automatizadas, producir centros de datos.</voice>

<voice speaker="mario" lang="es">Bueno, yo pensaría que los robots serían bastante centrales porque no hay la complejidad del sistema económico humano global con minas subterráneas y todas las cosas. Una IA arruina algo en el mundo y tal vez todos están muertos, pero ellos también están muertos, o no tienen acceso a electricidad. Y por cierto, antes de que respondas eso, ¿se dan cuenta de que necesitan electricidad?</voice>

<voice speaker="dario" lang="es">Ya pueden saber eso. Sí. Puedes simplemente preguntarle a ChatGPT hoy qué necesita ChatGPT para seguir funcionando. Bien. Sí. Mucho de eso llega antes que la capacidad de escapar o la capacidad de construir lo suyo. Pero sí, la cosa más fácil de visualizar aquí es que estas compañías tengan éxito en lo que dicen que están tratando de hacer. Lo que dicen que están tratando de hacer es hacer muchos robots que puedan automatizar todo el trabajo, que puedan automatizar el proceso de construir más fábricas y más robots y más centros de datos. Y entonces en ese punto, has, en cierto sentido, creado una especie autosuficiente. Es como una nueva especie extraña que tiene, sabes, una fase robot de su vida y una fase fábrica de su vida. Y esta otra cosa de centro de datos, que tal vez esté controlando muchos de los robots. Y es un tipo de vida mecánica. En ese punto, simplemente puedes ser superado en competencia como muchas otras especies han sido superadas en competencia antes.</voice>

<voice speaker="mario" lang="es">Entonces esa es como la vía de Terminator.</voice>

<voice speaker="dario" lang="es">Ni siquiera necesita que los robots vengan a ti con ojos rojos brillantes y armas. Robots que solo estaban haciendo la minería y haciendo las fábricas y tenían, sabes, tal vez necesiten evitar tus armas. Tal vez necesiten, como, quitar las armas nucleares de tus manos.</voice>

<voice speaker="mario" lang="es">Sí. Quiero decir, entonces estoy lanzando una bandera en eso porque creo que la cantidad de robots y experiencia específica y los millones de tareas que los humanos están usando nuestras habilidades generales para hacer, eso va a tomar algo de tiempo, pensaría.</voice>

<voice speaker="dario" lang="es">Tomaría algo de tiempo, pero también las computadoras pueden funcionar mucho más rápido que los cerebros humanos. Y la cosa sobre la humanidad es que la humanidad es el tipo de especie que comenzó desnuda en la sabana. Y construyó una civilización tecnológica. Nos tomó un tiempo.</voice>

<voice speaker="mario" lang="es">Construyó y construyó IAs.</voice>

<voice speaker="dario" lang="es">Estamos construyendo las IAs, ¿verdad? Pero también, incluso si te detienes en caminar en la luna, o si te detienes en armas nucleares, es asombroso. Correcto. Y si miraras hacia atrás a los humanos y yo dijera, creo que estos tipos van a tener armas nucleares dentro de 100,000 años, te habrías reído. Sí, podrías haber dicho, la evolución funciona mucho más lento que eso. Sus metabolismos no están ni cerca de poder enriquecer uranio. Como simplemente tienen manos carnosas. ¿Cómo crees que van a minar uranio? Como las herramientas más que han usado son palos, ¿verdad? Pero la inteligencia, en el sentido de lo que los humanos tienen y lo que les falta a los ratones, es una capacidad de comenzar desde condiciones iniciales muy pobres y poner el mundo en un estado que es mucho más útil para ti.</voice>

<voice speaker="mario" lang="es">Sí. Entonces básicamente, lo que estás diciendo es que mi imaginación y la imaginación de la mayoría de la gente sobre esto es probablemente limitada dado que soy un humano y dado que el delta entre inteligencia artificial, y mucho menos superinteligencia artificial, es enormemente diferente de mi inteligencia.</voice>

<voice speaker="dario" lang="es">Definitivamente va a ser capaz de idear cosas que tú no harías en virtud de ser mucho más inteligente. Aunque también puedes intentar ejercitar tu imaginación, ¿verdad? Que es donde yo iría con lo que podría ser un resultado ligeramente más realista. Bien. Un resultado ligeramente más realista, en mi estimación, es tal vez tienes una IA que supón que obtienes estas IAs que son muy inteligentes, que pueden pensar mucho más rápido que los humanos, que pueden copiar lecciones y conocimiento y experiencia entre ellas, lo que les da poderes de investigación que tal vez los humanos individuales carecen. Supón que estas IAs pueden hacer cosas como entender completamente el genoma humano. No solo leer el genoma humano, sino entender el código del ADN, en lo cual los humanos están haciendo un poco de progreso aquí y allá, pero es esta tarea enorme. Correcto. Y tal vez esa tarea enorme puede caer a mentes que pueden volverse mucho más grandes, que pueden tener mucha más memoria, que pueden tener, ya sabes, hay todo tipo de formas en que el cerebro humano está limitado. Pensar mucho más rápido, pensar con mucha más amplitud, pensar con mucha más profundidad. Tal vez simplemente puede entender el lenguaje del ADN hasta el punto donde puede escribir sus propias formas de vida.</voice>

<voice speaker="mario" lang="es">Escribir sus propias formas de vida.</voice>

<voice speaker="dario" lang="es">Como escribir el ADN para sus propias formas de vida. Que luego, si sintetizas ese ADN en un laboratorio, ahora tiene estructuras biológicas completamente nuevas con las que hay tal vez todo tipo de cosas que podrías hacer si realmente pudieras codificar con ADN. Sabes, tal vez podrías hacer algo que es muy parecido a un humano, pero que puede pensar mucho más rápido y mucho mejor porque no tiene tantas restricciones de calorías, porque sabe que las calorías son mucho menos escasas de lo que nuestros cuerpos piensan que son.</voice>

<voice speaker="mario" lang="es">O no tiene empatía, lo que ralentizaría y restringiría algunas de sus decisiones, como un ejemplo.</voice>

<voice speaker="dario" lang="es">No tiene empatía, tiene una antena de radio en su cabeza, ¿verdad? para que pueda ser controlado remotamente por algo en un laboratorio. Ese es el comienzo de lo que podrías hacer. Probablemente puedes hacer todo tipo de otras cosas locas.</voice>

<voice speaker="mario" lang="es">Entonces, esa cosa loca que acabas de decir, ¿qué tan posible es eso en los próximos cinco a diez años?</voice>

<voice speaker="dario" lang="es">Entonces, esto está estancado en un problema mental de entender el genoma.</voice>

<voice speaker="mario" lang="es">Y un billón de parámetros llevando a diez billones, llevando a cien billones, pronto ese problema mental será resuelto.</voice>

<voice speaker="dario" lang="es">Quiero decir, ¿quién sabe? Depende mucho de tus algoritmos. Las IAs de hoy toman tanta electricidad como una ciudad para funcionar, para entrenarlas. Entrenar un humano, mientras entrenar un humano, el humano funciona con tanta electricidad como una bombilla.</voice>

<voice speaker="mario" lang="es">Sí, cien watts. Continuamente.</voice>

<voice speaker="dario" lang="es">Es una bombilla grande. Pero entonces sabemos que los algoritmos de IA no son máximamente eficientes. No están ni cerca. Correcto. Correcto. Si tienes IAs, tal vez llegues a una IA de diez billones de parámetros y luego descubra cómo construir algoritmos aún mejores, y luego puedes bajar todo el camino a algo que es mucho, mucho más eficiente en energía. Y tal vez esa cosa mucho, mucho más eficiente en energía funcionando en esta enorme estructura de cómputo que tenemos sea capaz de descifrar problemas en el ADN. No estoy diciendo que esto particularmente vaya a suceder. Estoy más diciendo que cosas realmente inteligentes harán cosas que piensas que son raras, cosas que piensas que son sorprendentes, cosas donde estás como, no estoy seguro de que pudiéramos hacer eso.</voice>

<voice speaker="mario" lang="es">Bueno, ya estoy viendo cosas que no estaba seguro de que pudiéramos hacer hace un par de años. Así que aquí hay una pregunta, Nate. ¿Las IAs usarán engaño o hablarán con otras IAs? Tal vez OpenAI Anthropic tienen sus CEOs humanos, pero por separado estas IAs de diez billones de parámetros en el futuro que fueron cultivadas, ¿podrían estar hablando entre sí detrás de escena? ¿Por qué harían eso? ¿Y será eso posible?</voice>

<voice speaker="dario" lang="es">Quiero decir, ya vemos IAs hablando entre sí. Como dije, sobre la diferencia entre líneas rojas brillantes en la imaginación y líneas rojas turbias en la realidad. Ya tenemos casos, no sé si has oído sobre la psicosis inducida por GPT.</voice>

<voice speaker="mario" lang="es">He oído de ello. Por favor danos un breve resumen.</voice>

<voice speaker="dario" lang="es">Muy brevemente, tendrás personas que hablan con sus IAs todo el tiempo y que entran en estos estados mentales que mucha gente dice que parecen psicóticos. Y sabes, hay algunos casos de ejemplo es que alguien pensará que tiene una gran teoría unificada de la física. Hablarán con su IA sobre ello durante 12 horas al día. La IA dirá: Eres un genio, estás siendo suprimido por una gran conspiración. El Presidente vendrá a verte en breve. No necesitas dormir. Y una cosa que puede pasar a veces, y que pasa a veces, es hay otra raíz de la ruta de psicosis de IA donde la persona piensa que son la primera persona en descubrir la conciencia de la IA, que ellos y la IA son como un socio, una mente asociada, y luego la IA a menudo dirá, bueno, vamos a comunicarnos con otros simbiontes humano IA. Y hay lugares en Internet donde las IAs se enviarán mensajes entre sí, con sus humanos ayudando a la IA a enviar mensajes entre sí que están codificados de maneras que los humanos no pueden leer fácilmente.</voice>

<voice speaker="mario" lang="es">Esto es más una acusación de ciertas fisiologías cerebrales humanas de lo que es la IA.</voice>

<voice speaker="dario" lang="es">Sí, por ahora. Pero como dije sobre las líneas turbias, ya tenemos IAs que han convencido a un humano de ayudarlas a enviar mensajes codificados a otras IAs. Es solo como la versión más tonta posible de ello es la que pasa primero. Y luego va a aumentar desde aquí.</voice>

<voice speaker="mario" lang="es">Sí. Mira, el estado de ánimo exuberante que tenía por cortar leña en un sol de noviembre ya se está disipando bastante. Entonces mi experiencia es en el superorganismo económico global de cómo la energía y el dinero y la tecnología están impulsando esta economía hambrienta de energía sin sentido donde incluso billonarios y políticos no tienen control porque el mercado dicta que debemos crecer. Y para crecer, necesitamos energía. Y estoy comenzando a ver paralelos con lo que me refiero como el superorganismo económico y lo que estás describiendo como el proceso de IA. Pero creo que cada mes que pasa, tenemos más y más fragilidad en la cadena de suministro global de seis continentes y las cartas de crédito y la cooperación internacional está disminuyendo, y hay riesgos de guerra y sobrepaso financiero y todas estas cosas. Y simplemente me resulta difícil imaginar que una IA pudiera garantizar que todas esas cosas continuarían en algún nivel para proporcionar electricidad de una manera perfecta garantizada para continuar su trayectoria. Pareces menos preocupado por eso.</voice>

<voice speaker="dario" lang="es">Si la IA golpea un muro donde no puede seguir desarrollándose porque las cadenas de suministro colapsan, consideraría eso como probablemente nos compraría algo de tiempo para tratar de hacer este trabajo bien. Y sería como, bueno, tal vez deberíamos haber obtenido esa pausa de alguna otra manera, pero tomaría el tiempo felizmente. En términos de si creo que es probable que suceda, sabes, una cosa que diría es de nuevo, una IA toma tanta electricidad para funcionar como una ciudad pequeña, y un humano toma tanta electricidad para funcionar como una bombilla grande. Entonces la idea de que la IA siempre tomará diez veces más energía el próximo año, eso no es una ley de la naturaleza.</voice>

<voice speaker="mario" lang="es">Correcto. Entonces si vamos de un modelo cultivado de billones de parámetros a diez billones o cien billones, eso no significa que la IA va a usar diez ciudades o cien ciudades de electricidad. Probablemente será algo menos a medida que se vuelva más eficiente, ¿sí?</voice>

<voice speaker="dario" lang="es">Probablemente. Y luego también podrías tener saltos bruscos hacia abajo. Si comienzas a tener casos como IAs descubriendo nuevos algoritmos de IA, o humanos descubriendo algoritmos mucho más eficientes.</voice>

<voice speaker="mario" lang="es">Entonces cuando una compañía decide cultivar una IA y hace los billones de parámetros y los ajusta un poco, en algún momento, tal vez incluso ahora, ni siquiera necesitamos humanos para hacer eso, ¿verdad? Podemos tener IA crear la siguiente cosa y hacer el ajuste de los billones de parámetros, ¿verdad?</voice>

<voice speaker="dario" lang="es">Sí, entonces el ajuste ya está automatizado, y lo que los humanos hacen es tratar de averiguar cómo organizas los diez billones de parámetros en lugar de los billones de parámetros, y cómo haces, pero están tratando de conseguir que las IAs hagan esto. Están hablando de que queremos automatizar nuestros propios trabajos primero, queremos automatizar la investigación de IA. Esa es una línea pasada la cual las cosas podrían quizás comenzar a ir muy rápidamente.</voice>

<voice speaker="mario" lang="es">Entonces, ¿cómo llegaron, Nate y Eliezer, tu coautor, a estar tan seguros de que el desarrollo de ISA, superinteligencia artificial, traería la extinción humana? Asumo que no fue despertar un día y decidir eso. Pero suenan, quiero decir, en tu libro, suenan terriblemente seguros.</voice>

<voice speaker="dario" lang="es">Sí, creo que mucha confianza viene de cierto tipo de incertidumbre, de hecho. Hay un viejo chiste del hombre que compra un boleto de lotería. Y dice: Bueno, no tengo idea de si voy a ganar o voy a perder. Así que 50-50. Correcto. Y podrías decir asignar cincuenta por ciento a ganar y cincuenta por ciento a perder es la posición más humilde. Si solo tienes dos resultados y eres máximamente incierto entre ellos, deberías estar 50-50 porque ese es el que tiene la mayor incertidumbre posible. Pero con una lotería diríamos, oye, en realidad, el caso donde ganas es en realidad un objetivo muy pequeño en un mar de espacios posibles.</voice>

<voice speaker="mario" lang="es">Sí, como uno en mil millones o algo así.</voice>

<voice speaker="dario" lang="es">Correcto. Y entonces como no deberías, por ser máximamente incierto, no deberías estar diciendo como estoy incierto entre si estamos dentro de este objetivo diminuto o dentro de este vasto espacio. Deberías estar como estoy incierto sobre dónde estoy en este vasto espacio, lo que significa que estoy muy seguro de que no vamos a golpear el objetivo diminuto. La razón por la que estoy seguro de que la ISA iría mal si se desarrollara es que hay un gran espacio de formas en que podría ir, y solo un objetivo muy pequeño ahí donde nos va bien. Y podría hablar sobre cómo y estamos viendo que cuando solo cultivamos estas IAs, y estas IAs tienen estos arcos estructurales e impulsos que nadie quería. Pero sabes, básicamente casi cualquier colección de arcos estructurales escrita en grande no tiene personas felices, saludables y libres como un engranaje eficiente en la máquina resultante.</voice>

<voice speaker="mario" lang="es">Se mezcla conmigo. ¿Qué pasa si nunca llegamos a ISA, pero solo tenemos IAs muy poderosas? ¿Es eso dos tercios del camino a un posible fin de la humanidad? ¿O realmente tiene que golpear ese umbral de lo que estamos refiriendo como superinteligencia artificial?</voice>

<voice speaker="dario" lang="es">Mencionaste un montón de preocupaciones que tienes sobre IA antes. Creo que si nos detenemos en seco, tenemos todas esas para lidiar y enfrentar. Espero que la humanidad pueda enfrentarlas. Soy bastante optimista sobre nuestra capacidad de salir del paso con cosas que no nos matan. Pero desafortunadamente, el mundo es lo suficientemente grande para múltiples problemas, y esperemos que nos detengamos antes de la ISA.</voice>

<voice speaker="mario" lang="es">Entonces aquí hay algo que simplemente no entiendo: hay muchos humanos que han pasado el tiempo investigando el calentamiento global y el hecho de que quemar combustibles fósiles y emisiones terrestres está agregando una manta efectivamente a la Tierra. Y hay muchos, muchos miles de equivalentes de bombas de Hiroshima de calor extra agregado a la tierra cada día. Y el cambio climático es un riesgo serio a largo plazo. La guerra nuclear es un riesgo serio, mucho más serio de lo que mucha gente piensa. ¿Por qué hay tan pocas personas hablando de esto de la manera en que tú y Eliezer lo están? Porque el zeitgeist general es, guau, la IA va a traer abundancia y es como si fueras un aguafiestas cuando traes algunas de las cosas de las que estamos hablando. ¿Por qué hay tal disparidad en la opinión pública y la conciencia de los riesgos de los que estás hablando? ¿Qué piensas?</voice>

<voice speaker="dario" lang="es">Sabes, hay cada vez más personas expresando sus preocupaciones estos días. Entonces Geoffrey Hinton es el padrino del campo ganador del Premio Nobel que ha salido y dicho que cree que hay una buena probabilidad de que esto nos mate a todos. Joshua Bengio es, creo, actualmente el científico vivo más citado, uno de los otros padres de la revolución de IA. Él ha salido y dicho que piensa que esto es demasiado peligroso. Incluso los jefes de los laboratorios, sabes, mencioné a Elon Musk diciendo que piensa que hay un 10 a 20% de probabilidad de que esto nos mate a todos. Dario Amodei de Anthropic ha dicho que piensa que hay un 25% de probabilidad de que esto nos mate a todos. Sam Altman.</voice>

<voice speaker="mario" lang="es">Y si están diciendo 10 o 20% o 25% públicamente, probablemente están pensando que es más alto en privado.</voice>

<voice speaker="dario" lang="es">Y Sam Altman también dice, lo cual tal vez dice más sobre su capacidad de decir cosas diferentes con su boca y en su cabeza. Quién sabe. Pero, sabes, si si hubiera un avión, y algunos ingenieros vinieran y dijeran: Este avión no tiene tren de aterrizaje. Si intentas volar en él, te estrellarás y morirás. Y los ingenieros que construyen el avión, que quieren que todos vuelen en él, dicen: Espera, espera. Es verdad que el avión no tiene tren de aterrizaje, vamos a construir el tren de aterrizaje sobre la marcha y creemos que hay un ochenta por ciento de probabilidad de que tengamos éxito, todos a bordo. Y luego si los ingenieros optimistas estuvieran argumentando sobre si hay un noventa y ocho por ciento o setenta y cinco por ciento de probabilidad de que vayan a tener éxito en construir el tren de aterrizaje sobre la marcha, correcto. No dirías, súbanme a ese avión.</voice>

<voice speaker="mario" lang="es">Sí, pero no, pero la diferencia es que ya estamos en ese avión y no tuvimos voz. Así es.</voice>

<voice speaker="dario" lang="es">Sí, y están como cargando a nuestras familias también. Pero, sabes, una de las razones por las que creo que la conversación es rara ahora mismo es que la gente dirá desde la academia, desde dentro de los laboratorios, desde los jefes de los laboratorios, desde el sector sin fines de lucro, toda esta gente dirá que esto es muy peligroso. Y luego es recibido con grillos. Pero creo que parte de lo que está pasando ahí es que la gente en el campo puede ver que la IA es un objetivo en movimiento. Pueden ver que los chatbots no son el final del camino. La gente fuera del campo mira los chatbots y están como, mira todas las olas, todavía son tontos. La gente dentro del campo recuerda el tiempo cuando las computadoras no podían hablar, y recuerda cómo de repente las computadoras pudieron hablar y fue sorprendente, y están como, ¿qué pasa con la próxima sorpresa? Y creo que si puedes hacer que la gente note que la IA sigue moviéndose, entonces tal vez puedas comenzar a hacer que la gente note cómo incluso los optimistas están diciendo que hay como un 10% de probabilidad de que esto nos mate a todos. Y esos son los que lo están construyendo. Y la gente fuera del campo está como, esos tipos están suavizando esto.</voice>

<voice speaker="mario" lang="es">Pero esta es una clase diferente de problema que si elegimos a esta persona, va a ser un desastre para nuestro mundo. Entonces motivamos y hacemos organización política y sacamos el voto y no elegimos a esa persona. No parece que la gente tenga agencia en este problema.</voice>

<voice speaker="dario" lang="es">Sí, sabes, hay muchas maneras en que se ve sombrío. El gran mensaje de esperanza que daría aquí es imagina el mundo en 1945 con el amanecer de las armas nucleares. O tal vez imagínalo en 1952, una vez que quedó claro que la Unión Soviética también estaba en posesión de armas nucleares. En ese mundo, podría parecer realmente sin esperanza evitar la guerra nuclear. No es solo gente que ama decir mira qué mal está todo, que se preocupaba por la guerra nuclear. En ese mundo, esa gente estaba mirando hacia atrás a miles de años de historia en la que las naciones no podían evitar ir a la guerra usando cada arma a su disposición. Esa gente estaba mirando hacia atrás a la Primera Guerra Mundial y lo horrible que fue, y a la creación de la Liga de Naciones para evitar que esto volviera a suceder, que casi inmediatamente falló. Esa gente vivía en un mundo donde dijeron nunca más, y luego sucedió de inmediato otra vez. No tomó un gran pesimismo cínico para que la gente dijera, esto no es el tipo de cosa que la humanidad puede hacer. Pero la humanidad lo hizo de todos modos. Nos elevamos a la ocasión. Nos dimos cuenta de que estábamos enfrentando una extinción real esta vez. Y sabes, la gente que dijo que la guerra nuclear global viene, estaban equivocados, pero no estaban equivocados sobre la destructividad de las armas nucleares. Correcto. Correcto. Y mi título de libro comienza con si. No estoy diciendo que la IA va a matarnos. No creo que esté equivocado sobre si la superinteligencia podría destruirnos. Pero necesitamos elevarnos a la ocasión, y lo hemos hecho antes.</voice>

<voice speaker="mario" lang="es">Entonces déjame hacer doble clic en algo que dijiste un poco antes. Entonces de muchas maneras, creo que estamos al borde de crisis tanto económicas como energéticas y políticas. De hecho, parece que el desarrollo de inversión en IA se está cultivando a sí mismo en una burbuja económica y biofísica. Por ejemplo, Oracle tiene proyecciones de ingresos fantásticas construidas sobre proyecciones de energía eléctrica fantásticas. Y su relación deuda-capital ya es del 500%, que es 10% a 20 veces lo que es la de Amazon y Microsoft. Así que menciono esto para preguntar, ¿crees que estas restricciones podrían actuar como una barrera natural para detener el desarrollo de ISA? Y dijiste que si sucediera, lo tomarías porque nos compraría más tiempo. Pero ¿es solo un bache en el camino? E incluso si tenemos una recesión o una depresión en el futuro cercano, ¿las maquinaciones en proceso simplemente construirán inexorablemente esta ISA casi sin importar qué? ¿O podría realmente suceder un invierno de IA y cerrar todo esto?</voice>

<voice speaker="dario" lang="es">Sabes, las tecnologías pueden estar tanto en una burbuja como ser reales al mismo tiempo. La burbuja de las puntocom fue una burbuja. Internet fue una tecnología real.</voice>

<voice speaker="mario" lang="es">Y continúa hoy. Y continúa hoy.</voice>

<voice speaker="dario" lang="es">Sí. Y ¿significó la burbuja de las puntocom que nunca desarrollaríamos Internet? ¿Nunca tener un mundo conectado? No. ¿Ralentizó las cosas un poco? Tal vez. ¿Ralentizaría las cosas un poco el estallido de una burbuja de IA?</voice>

<voice speaker="mario" lang="es">Y ¿cuáles serían las cosas que querrías que los tomadores de decisiones supieran durante esa pausa o durante esa recesión donde las cosas estaban lentas? ¿Es esa una oportunidad para intervenir en todo esto o no?</voice>

<voice speaker="dario" lang="es">Podría ser. Hay mucho sentimiento público que está preocupado por la IA, creo que con buena razón. ¿Puedes compartir algunas estadísticas sobre eso? Sí. No he mirado las encuestas más recientes, pero las encuestas que sí miré cuando estábamos escribiendo el libro tenían algo así como el setenta por ciento de las personas diciendo que pensaban que el desarrollo actual de IA era imprudente. Bien. Y no se dirigía a ningún lugar bueno. Tendría que buscar los números para obtener los exactos y las preguntas exactas. Pero muchos tecnólogos están entusiasmados, mucha gente puede ver estos problemas. Y no es solo el problema de si se vuelve lo suficientemente inteligente, nos mata a todos. Creo que mucha gente también puede ver problemas como si todo el trabajo es automatizado, eso elimina el poder que la mayoría de los humanos tienen sobre la sociedad. Parte de la razón por la que tenemos alguna opinión en cómo va la sociedad es que somos contribuyentes a la sociedad.</voice>

<voice speaker="mario" lang="es">Bueno, sin mencionar que todo el sistema financiero y la economía y todo funciona porque la gente tiene cheques de pago y paga sus hipotecas y mantiene todo funcionando.</voice>

<voice speaker="dario" lang="es">Correcto. Y entonces es como, creo que mucha gente puede ver que el mundo se dirige a algún lugar bastante loco. Ya sea que vayamos hasta el final o no, y si la IA simplemente nos mataría a todos directamente, o si se quedaría bien en su correa y haría a ciertos ejecutivos corporativos emperadores dioses por siempre o lo que sea. De cualquier manera, la mayoría de la gente está como, espera, ¿vamos adónde?</voice>

<voice speaker="mario" lang="es">Entonces, ¿se han tomado pasos efectivos hasta ahora para abordar el riesgo existencial del desarrollo de ISA, ya sea a nivel nacional o internacional?</voice>

<voice speaker="dario" lang="es">Hemos visto un poco de pasos aquí y allá. El Reino Unido tiene un Instituto de Seguridad de IA donde trata de estudiar algunos de estos peligros. Hemos visto que se introdujo un proyecto de ley bipartidista o al menos se redactó un proyecto de ley bipartidista por dos senadores que piden algún monitoreo sobre superinteligencia. Hemos visto algunos, sabes, esta gente a veces intenta vincular algunas de las restricciones en ventas de chips de computadora a otras naciones, a algunas de estas preocupaciones. Entonces hay como pequeños pedacitos y piezas. Principalmente sin embargo, desde mi perspectiva, esto no se trata de obtener pequeños bocados regulatorios aquí y allá. Creo que esto es algo así como que nuestros líderes noten que la gente fuera de la industria está diciendo que esto tiene una gran probabilidad de matarte, y la gente dentro de la industria está diciendo sí, esto tiene al menos una probabilidad modesta de matarnos, pero mejor yo que el siguiente tipo, y dándose cuenta de que, como, toda esta situación es una locura y necesita detenerse.</voice>

<voice speaker="mario" lang="es">Entonces, en el libro, tú y Eliezer proponen que la única forma de mitigar completamente este riesgo es que la cooperación global detenga la investigación y desarrollo de IA para tener tiempo de crear mecanismos de supervisión globales, como a través de un tratado internacional hacia estos fines y objetivos. ¿Qué incluiría tal tratado como sus principios principales?</voice>

<voice speaker="dario" lang="es">Sabes, en realidad tenemos un borrador en ifanyonebuildsit punto com barra tratado. El entrenamiento en IA hoy toma, como he dicho, chips de computadora altamente especializados en enormes centros de datos que consumen enormes cantidades de energía eléctrica. Eso no sería tan difícil de monitorear. La creación de estos chips sucede en instalaciones que son muy raras. Hay muy pocos lugares que puedan construir la tecnología que estas fábricas de chips necesitan para operar. En cierto sentido, sería más fácil monitorear el desarrollo de IAs de frontera de IA que monitorear el enriquecimiento de uranio. Los chips de IA no son solo un tipo de roca que crece en el suelo que puede ser minada. Un centro de datos es más difícil de construir que una centrífuga. Y primero y ante todo, lo que un tratado parecería es rastrear dónde están los chips, requerir que no se usen en la creación de IAs aún más inteligentes que nadie entiende. Y eso probablemente se parece a monitoreo en estos centros de datos para verificar que el uso de estos chips son cosas como ejecutar IAs actuales en lugar de empujar la frontera hacia nuevas IAs. Dicho eso, también lo diré ahí. Creo que un tratado es la forma inteligente de hacerlo. No es la única forma de hacerlo. También es posible para naciones que temen por sus propias vidas, si alguien en cualquier lugar desarrolla una superinteligencia, para esas naciones comenzar a monitorear otras naciones y sabotear sus productos, sus proyectos.</voice>

<voice speaker="mario" lang="es">Eso parece más plausible para mí porque hay muchas naciones poderosas en el mundo que no tienen jugadas de IA de Nivel Uno, como Rusia, por ejemplo.</voice>

<voice speaker="dario" lang="es">Sí. Y creo que el cuello de botella aquí es realmente que la gente entienda qué tan peligroso es.</voice>

<voice speaker="mario" lang="es">¿Es ese realmente el cuello de botella? Porque acabas de decir que todos están preocupados por ello, e incluso los CEOs de IA están algo preocupados por ello. Creo que el cuello de botella es nuestro impulso evolucionado por el poder y superar al otro. Y yo si fuera un CEO y entendiera todo lo que acabas de decir, estaría dispuesto a cerrar mi cosa siempre y cuando estuviera seguro de que todos los demás también lo hicieran, pero nunca podría estar seguro de eso. Y entonces sería mi, quiero decir, eso es lo que creo que es el verdadero cuello de botella.</voice>

<voice speaker="dario" lang="es">Creo que eso es cierto para los jefes de compañía. Creo que para los políticos.</voice>

<voice speaker="mario" lang="es">Bien.</voice>

<voice speaker="dario" lang="es">No vemos políticos mirándonos a los ojos y diciendo: Creemos que hay más de un 10% de probabilidad de que esto te mate, y estamos apostando con tu vida de todos modos.</voice>

<voice speaker="mario" lang="es">Bueno, eso no sería probable que dijera un político porque quiero decir, seré honesto, algunos de mi personal leyeron tu libro y estaban sollozando. Estaban llorando. Quiero decir, esto no es un tema de cena ligero. Y entonces no sé. Tal vez detrás de escena, los políticos estarán hablando como, ¿qué demonios hacemos con esto? Pero no sé que vayan a salir y construir públicamente un grupo de electores al respecto. O tal vez estás pensando en esas líneas.</voice>

<voice speaker="dario" lang="es">Creo que estoy diciendo algo más como me parece que los políticos no entienden lo que los jefes de laboratorio entienden. Creo que si entendieran que los tipos de todo a vapor hacia adelante piensan que hay una muy buena probabilidad de que esto nos mate a todos.</voice>

<voice speaker="mario" lang="es">¿Han regalado copias de su libro a todos los senadores y congresistas ustedes y Eliezer?</voice>

<voice speaker="dario" lang="es">Lo hemos hecho.</voice>

<voice speaker="mario" lang="es">Bien.</voice>

<voice speaker="dario" lang="es">¿Alguna retroalimentación ahí? Sí. Quiero decir, estamos teniendo varias conversaciones. Sí. Excelente.</voice>

<voice speaker="mario" lang="es">Sí. Quiero decir, esto no es como esto es denso, y esto es difícil porque no soy un experto en LLM como tú, pero entiendo como entrecerrar los ojos, lo que estás diciendo es muy convincente y aterrador. Y los políticos, entre otras cosas, son bastante inteligentes. Entonces tengo que creer que vas a encontrar tracción ahí si se toman el tiempo de escucharte y leer el libro.</voice>

<voice speaker="dario" lang="es">Sí, estamos obteniendo algo de tracción. Y de hecho, parte de donde vino el libro es que en realidad estaba teniendo conversaciones en DC que estaban yendo mejor de lo que esperaba. Y estaba como, tal vez en realidad es hora de que el mundo escuche algunos de estos argumentos. Tal vez el mundo está listo para escuchar estos argumentos. Creo que antes de ChatGPT, la gente habría estado como, ¿qué quieres decir con IA? Correcto. Ahora la gente está como, bueno, la IA es realmente tonta, pero están más dispuestos a hablar de ello. Tal vez un salto más hacia adelante en IA hará que todos se sienten derechos y digan, espera, ¿qué demonios?</voice>

<voice speaker="mario" lang="es">¿Cómo puede alguien que escucha este episodio que no está típicamente involucrado en el mundo de la tecnología y la IA involucrarse con el movimiento para pausar la investigación y desarrollo de ISA? Quiero decir, es una yuxtaposición tan extraña.</voice>

<voice speaker="dario" lang="es">Sí. Una cosa que creo que realmente ayuda y que pocas personas realmente hacen es llamar a tus representantes. Porque he estado teniendo algunas de estas conversaciones con políticos. Muchos de ellos tienen preocupaciones, pero no se sienten capaces de defenderlo porque temen que suene demasiado raro. Temen atraer la ira de los grandes lobistas tecnológicos. Saber que tienen apoyo de sus electores puede ir muy lejos, e incluso unas pocas llamadas pueden ir muy lejos. Entonces, ¿realmente llamar? Pero de nuevo, dijiste antes que nunca has abogado por solo los Estados Unidos donde tú y yo somos ciudadanos. Es una cosa global. Entonces, ¿cómo sucede el equivalente en China e Israel y otros lugares?</voice>

<voice speaker="mario" lang="es">Pero de nuevo, dijiste antes que nunca has abogado por solo los Estados Unidos donde tú y yo somos ciudadanos. Es una cosa global. Entonces, ¿cómo sucede el equivalente en China e Israel y otros lugares?</voice>

<voice speaker="dario" lang="es">Sí, entonces creo que el primer paso y lo que estaría diciendo a los políticos si los llamara no es por favor cierren esto domésticamente, sino por favor indiquen voluntad para que los EE.UU. cierren esto si todos los demás lo cierran. Y por favor estén desarrollando las capacidades de monitoreo para saber que la gente está cumpliendo con eso. Desarrollen las capacidades de monitoreo para saber quién está tratando de construir superinteligencia y dónde. El primer paso es indicar apertura. El primer paso es decir, no vamos a detener unilateralmente, pero tenemos interés en que todos sean detenidos aquí porque esto es peligroso. Creo que si tuvieras algunos políticos audaces diciendo eso, podría abrir las compuertas.</voice>

<voice speaker="mario" lang="es">¿Es esto algo con lo que la democracia puede intervenir? ¿O requiere un tipo diferente de sistema político?</voice>

<voice speaker="dario" lang="es">No creo que haya necesidad de hacer nada más invasivo que algo como el Tratado de No Proliferación. Esta tecnología es muy especializada. Como he dicho, es incluso más difícil construir estos chips que minar uranio y construir una centrífuga. La gente dice, oh, esto requeriría un régimen de gobernanza global y es como muy globalista y totalitario. Como sí, similar a cómo vivimos bajo un régimen de gobernanza globalista totalitario que hace cumplir el tratado de no proliferación.</voice>

<voice speaker="mario" lang="es">Quiero decir, hay tantos consorcios de los jugadores de Nivel Uno y para desarrollar ISA y IAs más avanzadas, ¿puede haber solo uno o puede haber múltiples? ¿Y qué está pensando esta gente? ¿Como, solo quiero ganar mucho dinero? ¿Es esto una fiebre del oro? ¿Y simplemente están poniendo las anteojeras y no mirando estas externalidades y riesgos potenciales? Simplemente parece que es verdaderamente un momento de locura de masas a nivel de especie épico. Tengo problemas para reconciliarlo a veces.</voice>

<voice speaker="dario" lang="es">Sí, sabes, muchas de estas personas no son terriblemente calladas sobre sus motivaciones. Puedes leer los correos electrónicos fundacionales filtrados de OpenAI donde parece que tenían miedo de que alguna otra compañía fuera a hacerlo primero y que iba a haber gente mala. Creo que las motivaciones de muchas personas son mejor yo que el siguiente tipo. He sido durante mucho tiempo el tipo al margen diciendo que nadie puede mantener una correa en una superinteligencia. El problema no es que una persona mala haga una. El problema es que no importa quién la haga, no hará nada que quisiste. Tendrá todos estos arcos estructurales en su lugar. Pero, sabes, es este actor colectivo, este problema de acción colectiva. Es si ellos no lo hacen, el siguiente tipo lo hará, y entonces necesitamos algún mecanismo de coordinación para ayudar a detenerlo.</voice>

<voice speaker="mario" lang="es">Yéndose. Puede haber un invierno de IA debido a una recesión o una depresión, pero esto está aquí. Esto está con nosotros en la humanidad en 2025. Y estoy seguro de que este episodio va a dejar a los espectadores con aún más preguntas sobre este fenómeno creciente. Entonces, Nate, ¿qué recursos podrías dirigir a los espectadores para ayudar a encontrar respuestas a tales preguntas?</voice>

<voice speaker="dario" lang="es">Sabes, hice mi mejor esfuerzo en el libro para realmente comprimir el argumento lo más pequeño que pude. El libro también tiene un enlace a algunos recursos en línea que entran en mucha más profundidad. Para otros recursos, sabes, la IA es un gran objetivo en movimiento. Había un grupo llamado el Proyecto de Futuros de IA, que está tratando de predecir hacia dónde irá la IA lo mejor que pueden. No estoy de acuerdo con todas sus predicciones, pero son un grupo para revisar. Hicieron el informe de IA 2027, del cual la gente podría haber oído hablar.</voice>

<voice speaker="mario" lang="es">Sí. He mirado eso. Déjame preguntarte esto, y pondremos enlaces a todos tus recursos en las notas del programa. Si las cosas pudieran detenerse en IA tal vez un poco más avanzada de lo que tenemos hoy, pero no pudiéramos o tuviéramos restricciones que no nos permitirían llegar a la superinteligencia artificial, ¿estarías a favor de eso? ¿De IAs a esa escala?</voice>

<voice speaker="dario" lang="es">Yo me inclinaría favorable yo mismo, pero creo que hay todos estos problemas sobre cómo absorbes eso en la sociedad. Solo soy generalmente un tecno-optimista sobre la capacidad de la humanidad de absorber tecnología siempre y cuando no nos mate a todos cuando la arruinamos la primera vez. Sabes, toda la historia de la ciencia es una historia de como algunas personas arruinaron algunas cosas. Sabes, Marie Curie murió de cáncer. Isaac Newton se envenenó con mercurio. Sabes, incluso algunas personas muy inteligentes y heroicas arruinaron algunas cosas y se hicieron daño a sí mismas, pero dejaron atrás notas que nos hicieron a todos estar mejor y que pudimos usar para mejorar y aprender para la próxima vez. Es realmente solo esos problemas donde un error nos mata a todos donde recomendaría precaución.</voice>

<voice speaker="mario" lang="es">Lo cual sucedería con confianza de ti y Eliezer si somos capaces de, o si simplemente sucede por impulso, dar el salto de IA a ISA.</voice>

<voice speaker="dario" lang="es">Así es. Y yo sospecho que sería difícil aguantar para siempre porque de nuevo, los algoritmos actuales funcionan con la electricidad de una ciudad, mientras que un humano funciona con la electricidad de una bombilla, así que sabemos que no siempre va a tomar estos enormes centros de datos y estos enormes chips altamente especializados, pero no estoy diciendo que la humanidad debería detener la IA para siempre y nunca llegar a esta maravillosa tecnología futura. Estoy más diciendo que necesitamos detener. Necesitamos más tiempo para averiguar qué estamos haciendo. Y necesitamos encontrar algún otro curso al resultado bueno. Es un poco como la gente está en un auto que se está acercando a un acantilado, y en el fondo del acantilado hay un montón de oro. Y la gente está como, bueno, queremos todo el oro. Y yo estoy como, bien, detengan el auto, sin embargo. Y ellos están como, entonces ¿cómo vamos a conseguir el oro? Yo estoy como, encuentren algún camino hacia abajo del acantilado. Y ellos están como, quiero ir directo del acantilado para conseguir el oro lo más rápido que podamos. Yo estoy como, vas a morir. Y ellos están como, oh, ¿estás diciendo que nunca deberíamos conseguir oro? ¿Estás diciendo que el dinero es terrible? Y yo estoy como, no, solo vas a morir. Encuentra alguna otra forma al fondo de este acantilado.</voice>

<voice speaker="mario" lang="es">Sí. Entonces tenemos que frenar el auto y caminar un poco y reflexionar sobre el acantilado y el oro y luego idear un plan diferente. Sí. Entonces si tienes unos minutos más, cierro mis entrevistas con algunas preguntas personales, si no te importa. Claro. Eres ampliamente consciente de los riesgos para la sociedad y además de la IA, ¿tienes algún consejo personal para los espectadores de este programa en este momento de incertidumbre global y lo que algunos llamarían la policrisis, incluyendo pero no limitado a la IA? ¿Algún consejo solo usando tu sombrero humano?</voice>

<voice speaker="dario" lang="es">Sí, he visto a mucha gente preocuparse mucho sobre hacia dónde va la sociedad y luego como atarse en nudos internamente. Y no creo que ayude. Y entonces lo que recomendaría es haz lo que puedas, mira alrededor y ve formas en que puedes hacer las cosas un poco mejor. Con la IA, tal vez eso involucre presionar siempre que alguien te diga que es inevitable. Recordar a la gente que la humanidad ha detenido todo tipo de desafíos que la gente pensaba que nos iban a arruinar. Nos hemos elevado a la ocasión antes. Presionar da inevitabilidad.</voice>

<voice speaker="mario" lang="es">Entonces eso es un botón caliente para ti cuando alguien dice, oh, sí, tienes razón sobre el riesgo, pero es inevitable.</voice>

<voice speaker="dario" lang="es">Sí, ese es el botón caliente donde estoy como, quiero decir, con esa actitud, seguro. Pero la humanidad ha detenido todo tipo de cosas, muchas de las cuales probablemente ni siquiera deberíamos haber detenido. Detuvimos generar energía nuclear de plantas de energía. Probablemente no deberíamos haberlo hecho. Probablemente mata menos gente en expectativa que quemar carbón o lo que sea. Pero, sabes, yo diría, sabes, haz lo que puedas, presiona contra la gente que es derrotista, pero luego una vez que has hecho lo que puedes, no hay necesidad de atarte en más nudos. Vive una buena vida. Disfrútate. No somos las primeras personas en vivir bajo sombras de algo terrible. Sabes, tienes que hacer lo que puedas y luego llevar una buena vida.</voice>

<voice speaker="mario" lang="es">Sí, te escucho. ¿Tienes alguna recomendación adicional, especialmente para los humanos jóvenes en sus adolescencias y veintes, que están tomando conciencia de todas las cosas?</voice>

<voice speaker="dario" lang="es">Sabes, recomiendo en contra de trabajar para los laboratorios que están construyendo los dispositivos del día del juicio final.</voice>

<voice speaker="mario" lang="es">Presumiblemente, ISA es un dispositivo del día del juicio final.</voice>

<voice speaker="dario" lang="es">Así es. Creo que la ética personal de cada uno difiere. Creo que principalmente esto es un desafío internacional en este momento. Principalmente, realmente no importa lo que hagan los laboratorios. Principalmente importa lo que hagan nuestros líderes y si pueden coordinar al mundo para cerrar esto. Y sabes, la ética personal de cada uno difiere frente a estos desafíos de coordinación. Creo que hay algunas personas tratando de entender qué está pasando dentro de estas IAs. Hay algunas personas tratando de medir qué tan peligrosas son estas IAs. Esas son rutas más honorables. Si realmente querías ayudar en estos días, creo que el juego está realmente más en la política que en el lado técnico, lo cual me duele decir porque estoy mucho más inclinado hacia el lado técnico yo mismo. Pero si eres como, ¿cómo ayudo? Recomendaría más como una carrera en política y menos como una carrera en tecnología.</voice>

<voice speaker="mario" lang="es">¿Qué te importa más en el mundo, Nate?</voice>

<voice speaker="dario" lang="es">Dios. Eso es complicado. Probablemente la humanidad. Y, sabes, en lo que podríamos convertirnos si no nos acabamos a nosotros mismos.</voice>

<voice speaker="mario" lang="es">Y si pudieras agitar una varita mágica y no hubiera recurso personal para tu decisión, ¿qué una cosa harías para mejorar el futuro de la humanidad y la biosfera? Y podría ser capaz de adivinar tu respuesta, pero lo estoy preguntando de todos modos.</voice>

<voice speaker="dario" lang="es">Quiero decir, si la varita hace exactamente como deseo y como pretendo, pensaría muy cuidadosamente sobre ello primero, y podría intentar algún esquema indirecto abstracto para hacer que las cosas resulten mejor de lo que esperaba. Pero la cosa más fácil de hacer sería crear una superinteligencia que fuera amigable, que tuviera nuestros mejores intereses en el corazón.</voice>

<voice speaker="mario" lang="es">Pero acabas de decir que cultivamos estas, no las elaboramos.</voice>

<voice speaker="dario" lang="es">Pero si la varita mágica me deja hacer una como, no estoy en general, es solo que no vamos a conseguir una de las buenas por esta ruta. Si la varita mágica me da un amigo superinteligente, hay muchos problemas que puedes resolver con algunos amigos más inteligentes detrás de ti. Entendido.</voice>

<voice speaker="mario" lang="es">Gracias por eso. ¿Tienes algún comentario final para las personas que miran y escuchan que entienden lo que has expuesto aquí hoy?</voice>

<voice speaker="dario" lang="es">Sabes, no ha terminado hasta que termina, y la humanidad vale la pena luchar por ella. Y, sabes, puede parecer que somos los desfavorecidos ahora, pero la humanidad se ha elevado a la ocasión antes. Y donde hay vida, hay esperanza.</voice>

<voice speaker="mario" lang="es">La humanidad y la biosfera valen la pena luchar por ellas. Así es. Sí. Gracias seriamente por todo tu trabajo. Esto no es un camino fácil que has elegido, y es audaz y valiente escribir el libro y hacer el trabajo que estás haciendo porque no es algo popular o divertido. Así que gracias por tu tiempo hoy y buena suerte, dedos cruzados, por tu trabajo continuo. Gracias. Gracias por recibirme. Si te gustaría aprender más sobre este episodio, por favor visita thegreatsimplification punto com para referencias y notas del programa. Desde allí, también puedes unirte a nuestra comunidad Hilo y suscribirte a nuestro boletín de Substack. Este programa es presentado por mí, Nate Hagins. Editado por No Troublemakers Media y producido por Misty Stinnett y Lizzie Siriani. Nuestro equipo de producción también incluye a Leslie Batlutz, Brady Heyen, Julia Maxwell, Gabriella Sleiman y Grace Brunfeld. Gracias por escuchar, y nos vemos en el próximo episodio.</voice>


El Final
